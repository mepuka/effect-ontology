Using LLMs for OWL Ontology Extraction and Generation

Introduction

Large Language Models (LLMs) are increasingly applied to automate ontology engineering tasks that traditionally required expert effort. Two emerging applications are: (1) using LLMs to extract knowledge from unstructured text and populate an existing OWL ontology (knowledge graph population), and (2) using LLMs to generate new OWL ontologies from natural language descriptions or requirements. Recent research and practice show that LLMs can accelerate these workflows by generating classes, properties, and triples from text, though careful prompt design and validation are essential to ensure formal correctness and completeness. Below, we survey the state-of-the-art techniques, prompt patterns, tools, and evaluation strategies for both ontology population and ontology generation, with examples in general-purpose domains (e.g. social sciences, education, general knowledge).

LLM-Powered Ontology Population from Text

Task Overview: Ontology population is the process of extracting entities and relationships from unstructured text and instantiating them as individuals and facts in an existing ontology schema ￼ ￼. For example, given a defined ontology of historical persons, an LLM can read a biography and produce RDF/OWL triples that add facts about a person’s birthdate, relationships, etc. This task leverages the natural language understanding of LLMs to interpret text and map it into the ontology’s classes and properties.

Prompting Techniques: A common prompting pattern is to instruct the LLM to output triples in a specific format (Turtle or RDF serialization), given the text and a description of the ontology schema. For instance, one workflow uses a system message like “You are an expert in ontology. Extract RDF triples from the following text in Turtle format, adhering to the ontology.” and provides a list of ontology classes/properties as context ￼ ￼. The LLM then returns triples conforming to that schema. An example from a healthcare domain prompt is:
• System prompt: “Extract RDF triples in Turtle format, using the following ontology schema: Patient (properties: hasName, hasAge, hasGender, hasCondition, treatedBy), Doctor (properties: hasName), Condition (properties: hasName).”
• User prompt: “Text: ‘Patient John Doe, aged 45, was diagnosed with diabetes and treated by Dr. Smith.’ Generate RDF triples.”

The LLM output in this case was:

@prefix : <http://example.org/healthcare.owl#> .

:JohnDoe a :Patient ;
:hasName "John Doe" ;
:hasAge 45 ;
:hasCondition :Diabetes ;
:treatedBy :DrSmith .

:Diabetes a :Condition .
:DrSmith a :Doctor ;
:hasName "Dr. Smith" .

￼ ￼. This illustrates how the prompt guides the model to produce OWL individuals and facts aligned with the provided ontology vocabulary.

To improve accuracy, few-shot prompting can be used by providing one or two examples of text-to-triple extraction in the prompt. Another effective strategy is decomposing the task: e.g. first ask the LLM to summarize or identify relevant facts in the text (possibly per ontology module), then in a second prompt ask it to format those facts as triples. In a case study populating a historical Enslaved Persons ontology, researchers first had the LLM summarize large Wikipedia articles into concise summaries focused on the ontology’s modules (using few-shot examples of summarization) ￼ ￼. They then prompted the LLM with those summaries plus the schema to generate triples. This two-step retrieval + generation approach ensures the LLM concentrates on relevant information even when source documents are lengthy ￼ ￼. Alternatively, a Retrieval-Augmented Generation (RAG) pipeline can be employed: relevant text chunks are fetched via semantic search (using embeddings) and appended to the prompt so the LLM only sees pertinent context ￼ ￼. In the Enslaved.org example, the team used LangChain to integrate a vector database for this purpose, retrieving text about a specific person before triple generation ￼. Prompt variants were tested – one explicitly told the LLM which entity the document is about, and another left it open – to see which yields better extractions ￼ ￼.

Use of Tools and Libraries: Implementations often rely on Semantic Web libraries to prepare and validate data. For example, before prompting the LLM, existing ontology data can be loaded and manipulated with RDFLib or Owlready2. In one pipeline, an OWL ontology file is imported via Owlready2 so that its classes and properties can be programmatically listed and fed into the prompt for context ￼ ￼. After the LLM produces triples, RDFLib’s parser can convert the Turtle output into a graph and check it against the ontology. This allows automatic detection of any triples that don’t conform to the ontology schema (e.g. using a property not defined in the ontology, or an object that is of the wrong type) ￼ ￼. For instance, one validation script iterates through each triple and checks that the predicate exists in the ontology and that the object’s type matches the property’s expected range (e.g. if hasAge expects an xsd:integer but the model gave a string) ￼ ￼. Detected errors can then be used as feedback to the LLM in a refinement prompt. The prompt might say: “The following RDF output has errors: … (lists triples and errors). Refine the triples to fix these issues while adhering to the ontology schema.” ￼. This iterative refinement loop (validate → feedback → re-prompt) helps ensure the final triples are consistent with OWL constraints ￼.

In terms of integration frameworks, LangChain is often used to orchestrate multi-step pipelines. It can handle the RAG flow (document chunking, embedding, retrieval) and manage calls to the LLM for each step (summarization, triple generation, refinement) ￼ ￼. LangChain provides a convenient interface to embed the ontology schema and context into prompts and to call LLM APIs like GPT-4 or open-source models. On the ontology side, engineers use tools like Protégé to define the initial ontology schema (classes/properties) and to manually inspect the populated knowledge graph after LLM output. Protégé isn’t called by the LLM directly, but it’s useful for reviewing the triples produced and running reasoners or the Ontology Pitfall Scanner on the results.

Ensuring OWL Compliance: A core challenge is preventing the LLM from producing invalid or nonsensical ontology content (a known risk since LLMs may hallucinate facts or misinterpret schema). Several strategies address this:
• Prompt constraints: The instructions explicitly remind the model to only use the ontology’s vocabulary and format. In the earlier example, the system prompt enumerated the allowed classes and properties to constrain the output ￼ ￼. Similarly, researchers provided module-specific templates such as “For relation hasAgeValue(Agent, xsd:double), an example triple is hasAgeValue(Absalom Jones, 71).” to guide the syntax ￼ ￼. By showing the exact expected format (including OWL datatypes and class names), the LLM is less likely to deviate. Prompts also warn against certain pitfalls – for example, one ontology-engineering prompt explicitly told the model not to produce an empty answer or a conversational reply, which are common failure modes when an LLM is uncertain ￼ ￼.
• Structured outputs: Requesting output in a formal serialization (Turtle, RDF/XML, JSON-LD) helps with automatic verification. If the LLM’s output is parseable by an OWL API without errors, that’s a good indicator of basic compliance. Conversely, if the output is invalid syntax, the pipeline can detect that and re-prompt. Many implementations set the LLM’s temperature to 0 (fully deterministic) to reduce randomness in syntax and naming ￼. This means if a class like Patient was mentioned in the prompt, the model is more likely to reuse the exact token “Patient” rather than a creative variant.
• Human or rule-based validation: As mentioned, using a validator (script or tool) to check each triple against the ontology schema is critical ￼ ￼. Some projects also run the OOPS! (Ontology Pitfall Scanner) on the output to catch semantic issues. OOPS! can flag things like missing rdfs:label annotations, unattached classes, or improper hierarchy, which an LLM might not consider. In one study, after populating an ontology with LLM, the authors ran OOPS! and a structural consistency check to identify superfluous elements (e.g. extra classes or properties that were not in the requirements) ￼. They found LLMs sometimes introduce redundant elements or minor inconsistencies that need pruning ￼. In practice, domain experts might review a sample of the extracted triples for quality, especially if the KG will be used in a high-stakes application.

Example Applications: In the social sciences/humanities domain, a notable example is the Enslaved.org knowledge graph population. Researchers used LLMs to extract biographical facts about enslaved individuals from historical text and populate a rich ontology (covering people, events, relationships) ￼ ￼. They reported that GPT-4 could recover a large portion of the ground-truth triples when guided by a modular ontology prompt, though it required careful schema simplifications to avoid confusing the model ￼ ￼. Another study evaluated LLM-based extraction on news articles (general knowledge domain): they found the LLM did well in identifying entities (classes and instances) but struggled with modeling relations correctly and maintaining logical consistency ￼ ￼. In the education domain, Li et al. (2024) used LLMs to extract key concepts from university lecture notes and automatically build a domain ontology; when integrated into a student model, the generated ontology improved performance predictions, demonstrating the practical value of LLM-derived ontologies in non-technical fields ￼. A recent pilot in social science literature reviews showed that LLMs can also assist in ontology-based data extraction for research synthesis: the team prompted GPT-4 and Mistral (open model) to identify sections of papers according to an ontology of document structure (DoCO ontology) and extract specific data points, evaluating the results against manually extracted data using overlap metrics (e.g. ROUGE, cosine similarity) ￼ ￼. These examples underscore that with the right prompts and checks, LLMs can populate ontologies across diverse domains, albeit typically with a human-in-the-loop to correct mistakes.

Evaluation Methods for Population: The quality of ontology population is usually evaluated by comparing the LLM-extracted facts to a reference or gold standard. This can be done at the triple level using precision, recall, and F1 scores of matching triples. Because exact string matching is often too strict (LLM outputs might use synonyms or minor format differences), string similarity metrics or embeddings-based scores are employed ￼ ￼. For example, one study converted both the ground truth and LLM output to TSV files and computed metrics like Jaccard similarity and Levenshtein distance on subject/predicate/object strings to see how closely the LLM’s triples matched the true data ￼. Another common evaluation is requirement coverage – if the ontology has competency questions or specific fields of interest, did the LLM manage to populate information for all of them? For the Enslaved KG, researchers measured the “proportion of competency questions answered” by the triples produced ￼. Finally, qualitative evaluation by domain experts is invaluable: experts can examine whether the triples make sense and are free of critical errors. If an LLM misses some facts but adds others, expert judgment is needed to decide if those additions are valid or hallucinated. In summary, a combination of automated metrics (to quantify overlap/correctness) and expert review (to catch subtle errors and judge usefulness) is used to assess ontology population results ￼ ￼.

LLM-Powered Ontology Generation from Natural Language

Task Overview: Beyond populating existing schemas, LLMs can assist in creating new OWL ontologies from scratch, using natural language inputs like domain descriptions, competency questions (CQs), user stories, or other unstructured requirements. The goal here is to have the LLM draft the classes, properties, and axioms of an ontology that faithfully represent the knowledge described in text. This can dramatically speed up the ontology engineering process by providing an initial OWL draft for human ontologists to refine ￼ ￼.

Prompting Techniques and Patterns: Crafting the right prompt is crucial to get structured and correct ontology content. Techniques range from simple single-shot prompts to complex multi-turn strategies:
• Direct Schema Prompts: For relatively straightforward domains, users have given LLMs a high-level domain brief and asked for an OWL ontology. A typical system prompt might be: “You are an expert ontology engineer. Generate an OWL ontology (in RDF/XML or Turtle syntax) for the following domain. Include appropriate classes, and for each object/data property specify domains and ranges.” ￼. The user then provides a paragraph describing the domain. For example, a prompt description: “Domain: In a healthcare scenario, Patients have a name, age, and gender; Patients can have Conditions and are treated by Doctors; each Condition has a name; Doctors have a name and a specialty.” From this, GPT-4 was able to output an OWL snippet defining classes Patient, Doctor, Condition and properties like hasName (datatype property) and treatedBy (object property linking Patient to Doctor), complete with <rdfs:domain> and <rdfs:range> for each property ￼ ￼. This one-shot approach leverages the LLM’s learned knowledge to propose a plausible ontology. However, it may miss nuances or require further prompts to add constraints (like cardinalities or hierarchies) and to correct any logical errors.
• Decomposed (Step-by-Step) Prompts: More advanced methods treat ontology generation as a multi-step dialogue or chain-of-thought process to improve completeness and coherence ￼ ￼. A leading example is the CQ-by-CQ method, where the ontology’s competency questions are tackled one at a time. In the Memoryless CQ-by-CQ variant, the prompt is structured to focus the LLM on one competency question and the accompanying user story (scenario) at a time, asking it to generate the ontology fragment that would answer that question ￼ ￼. The LLM is instructed to “act as an ontologist” and given guidance like Turtle syntax rules and reminders of common pitfalls (e.g., “Do not output an empty ontology. Don’t switch to conversational mode.”). It produces classes and properties relevant to that single competency question. Then the process moves to the next question. All partial outputs are finally merged into one OWL model ￼ ￼. Importantly, Memoryless CQ-by-CQ does not feed the LLM the previous questions or its earlier outputs during each step, to avoid context overflow and potential distraction ￼. This was found to reduce contradictory or irrelevant modeling – any overlaps or conflicts in the merged ontology can be resolved by a human afterward, which is easier than preventing the LLM from ever producing any overlap ￼ ￼.
Another sophisticated prompting strategy is Ontogenia, which combines chain-of-thought (CoT) reasoning with ontology design patterns. Ontogenia prompts the LLM in an iterative fashion, providing it with the current state of the ontology after each CQ and asking it to extend it with the next competency question’s knowledge ￼ ￼. It effectively simulates an expert’s methodology: (i) Interpretation phase: the LLM reads the user story and CQs, and identifies key concepts (classes) and relationships needed (this may involve the LLM “thinking aloud” to break down the question) ￼; (ii) Extension phase: the LLM reflects on whether additional axioms like restrictions or rules are needed to answer the CQ and incorporates those; (iii) Validation phase: after proposing an ontology update, the LLM is prompted to double-check its work, possibly by explaining how the CQ is now covered or by creating a small example to test the ontology ￼. Ontogenia also explicitly asks for richer OWL features – e.g. add inverse properties, rdfs:labels, and individual examples for each class – to make the ontology more complete ￼. Moreover, it injects relevant Ontology Design Patterns (ODPs) into the context (from a library like ontologydesignpatterns.org) to guide the model toward well-known modeling solutions ￼ ￼. For instance, if an ODP for a part-whole relationship or a time interval is provided (as a template of classes/properties), the LLM can reuse that structure in the output instead of inventing an ad-hoc pattern. Empirically, this method produced richer and more correct ontologies, as it encourages the model to reason about the ontology design at each step ￼. The trade-off is longer prompt sequences and the need to carefully handle the growing context (Ontogenia must supply the previous ontology content back to the LLM each iteration, which works best with models having large context windows).
• Role-playing and Schema Examples: Many prompts set a persona for the LLM (e.g., “You are an ontology engineering assistant”) and include format examples or schemas. For example, a prompt might show a mini ontology example like: “Class: Animal; Class: Dog subclassOf Animal; ObjectProperty: hasOwner domain Dog range Person.” as a guide. By providing a schema example, we prime the LLM to output ontology elements in a similar list or Turtle format. In one study, authors pre-loaded a gold standard ontology (African Wildlife Ontology) into the prompt as a reference, and the LLM managed to mimic its style to generate a new ontology ￼ ￼. However, care must be taken not to exceed token limits; sometimes only a relevant excerpt or just design patterns are given instead of a full ontology.

Ensuring OWL Compliance in Generation: When LLMs generate a brand-new ontology, there are additional compliance concerns beyond syntax – we need semantically sound modeling. Strategies include:
• Explicit format instructions: Similar to the population case, prompts insist on a certain serialization (OWL2 XML, Turtle, Manchester syntax, etc.). If the output can be parsed by an OWL API, that confirms basic syntactic validity. Some workflows prefer Turtle for readability, others RDF/XML if they want to ensure all OWL axioms are explicitly closed with proper tags. The LLM should be reminded to include ontology headers/prefixes if needed (many models will include common XML namespaces automatically as they’ve seen them in training data). In practice, minor syntax errors like unclosed tags or prefix typos can occur; these are usually caught and fixed either by the model on a second try or manually. Setting the system prompt to “Follow OWL syntax precisely” can reduce such errors.
• Use of Upper Ontologies or Standards: Guiding the LLM to align with an existing upper ontology (like BFO or schema.org) can improve compliance and interoperability. For example, Benson et al. (2024) experimented with prompts that require BFO-compliant class hierarchies, essentially telling ChatGPT to only create classes that fit under BFO’s top-level categories (Entity, Continuant, Occurrent, etc.) ￼ ￼. The LLM was able to produce OWL classes that were consistent with BFO’s worldview, though this approach was tested on only a few examples. The benefit is that the resulting ontology adheres to known ontological distinctions (important in life sciences and other fields), but the prompt must supply at least a summary of the upper ontology to be effective. Another approach is including Ontology Design Patterns (as in Ontogenia) to enforce best practices. By giving the LLM a template to follow (e.g., how to model a list, or a spatial containment pattern), we ensure the output is not only syntactically correct but follows community norms for modeling that situation ￼ ￼.
• Automated reasoning checks: After generation, the ontology can be loaded into Protégé and a reasoner (e.g., HermiT or Pellet) can run to detect logical inconsistencies or unsatisfiable classes. For instance, if the LLM accidentally made a class both a subclass of two disjoint classes, a reasoner would flag an inconsistency. There isn’t evidence of current pipelines having the LLM fix such logical issues automatically, but a human expert can adjust the axioms in Protégé. In principle, one could describe the inconsistency to the LLM and ask for a revised ontology, similar to the refine loop for data, but careful with trust.
• Human-in-the-loop review: Given that ontology modeling has many subtle choices, the LLM’s output is usually treated as a draft. Ontology engineers or domain experts then review it, adding or removing axioms. Notably, a 2025 study found that GPT-4’s ontologies (using the Ontogenia prompt) were good enough to meet many requirements, even outperforming novice human modelers, but they still contained some mistakes and variability in quality ￼ ￼. The safest practice is to use LLM output as a starting point and involve a human for final validation.

Tools and Frameworks: The generation process can be implemented with generic LLM SDKs (OpenAI API, etc.), but there are also emerging tools:
• LangChain or custom scripts to manage multi-turn prompts (especially for CQ-by-CQ or Ontogenia techniques). LangChain enables setting up a chain where Step 1 reads the requirements and calls the LLM, Step 2 feeds the output plus next input back in, and so on. It can also integrate tools (like calling an OWL reasoner or validator in between prompts if needed).
• Protégé and OWL APIs are used after generation. For example, the Medium pipeline (above) loaded the generated OWL into an Owlready2 Ontology object to print its contents and verify the classes/properties ￼ ￼. Protégé can be used to visualize the class hierarchy and manually tidy up the ontology. Some projects also output competency questions or descriptions alongside the OWL, and use those in Protégé’s annotation fields for documentation.
• GitHub toolkits: There are open-source projects aiming to streamline ontology generation. One such toolkit is OntoGenix ￼, which provides a GUI for going from a CSV dataset to an OWL ontology using GPT-4. OntoGenix guides the user through prompt crafting, automatically suggests an ontology schema based on data statistics, and then calls the LLM to produce the OWL classes/properties (it even handles RML mapping generation) ￼ ￼. Under the hood it likely uses a sequence of prompts: first to summarize the dataset and propose an ontology design, and next to generate the detailed OWL code for classes and their restrictions ￼ ￼. They provide example prompt templates, such as instructing the LLM to “explicitly reference all object and data properties for the entity {EntityName}, using OWL restriction syntax (use onClass for object property restrictions and onDataRange for datatype restrictions)” ￼. This level of prompting ensures the model knows to include domain/range restrictions in OWL notation. Such toolkits underscore practical considerations like connecting LLMs to actual data sources (CSV) and letting users intervene (e.g., manually edit a suggested ontology description before final generation).

Another example is the LLMs4OL (Large Language Models for Ontology Learning) Challenge, which has motivated development of workflows for automated ontology construction across domains ￼ ￼. Participants explored prompt-based methods versus fine-tuning. One entry (Fridouni et al. 2025) specifically compared Memoryless CQ-by-CQ and Ontogenia prompting on multiple domain texts and found that commercial LLMs can outperform human novices in building ontologies, though the outputs often needed manual refinement due to issues like redundant classes or minor structural errors ￼ ￼. This indicates that prompt-engineering approaches are viable, but not yet perfect. Meanwhile, other research is investigating fine-tuning LLMs for ontology tasks (e.g., Luo et al. 2023 fine-tuned a model for taxonomy induction and reported better consistency than prompting alone ￼). For general-purpose use in social sciences or education, however, prompt-based methods (which don’t require custom model training) are more accessible.

Evaluation of Generated Ontologies: Assessing an automatically generated ontology’s quality and completeness involves multiple criteria:
• Competency Question Coverage: Does the ontology answer the questions or fulfill the requirements it was supposed to? This is often measured by checking each requirement (CQ or user story) against the ontology. If a CQ asks “Can we represent that a doctor treats a patient?”, the generated ontology should have a treatedBy or equivalent relationship between Doctor and Patient. Researchers Lippolis et al. measured the proportion of CQs that were successfully modeled by the LLM’s ontology ￼. They had human evaluators (ontology engineers) manually verify for each CQ whether the ontology contained the classes/properties needed to answer it ￼. A high coverage (close to 100%) means the ontology meets its requirements; lower coverage indicates omissions.
• Ontology Quality Metrics: Standard ontology quality metrics can be computed, such as OOPS! pitfall scores, counts of classes, hierarchy depth, etc. In one experiment, after generating ontologies with GPT-4, the authors ran the Ontology Pitfall Scanner and reported issues like missing inverse relationships or unconnected classes ￼ ￼. They also did a structural analysis: comparing the number of extraneous elements (ones that were not demanded by any requirement) across different prompting methods ￼. Ontologies with fewer superfluous classes or properties were rated as better because they were more focused. Another metric is consistency – an ontology with no logical contradictions passes this (all classes can have models). Additionally, one can look at human readability: Are class and property names intuitive and are there rdfs:labels and comments? Ontogenia’s approach of asking for labels and comments is aimed at this ￼.
• Expert qualitative evaluation: Ultimately, domain experts often review the ontology to judge if it makes sense and if it would be usable for the intended purpose ￼ ￼. In the ESWC 2025 study, experts compared the LLM-generated ontologies to those created by student modelers, and noted that GPT-4 (with advanced prompting) produced very reasonable ontologies, though occasionally with slight variations in modeling choices ￼ ￼. Experts can catch nuances like whether a relationship should be transitive or if a concept should be split into two classes – things that automated metrics might not cover.

When evaluating LLM-generated ontologies in less technical domains (e.g. a social science ontology of cultural concepts, or an educational ontology of course topics), completeness might be measured against a reference ontology if one exists, or via user studies (asking end-users if the ontology captured the key concepts they expected). For example, if an LLM generates an ontology of news events from a corpus of articles, one could compare it to a manually curated ontology of events for coverage, or have journalists assess if important event attributes are present. Additionally, the usability of the ontology in an application (like search or prediction) can serve as an evaluation: Li et al. (education domain) evaluated the ontology by integrating it into a student performance prediction model and showed it improved the model, thereby validating that the ontology captured useful pedagogical structure ￼.

Conclusion and Resources

Using LLMs for ontology engineering is a fast-evolving area bridging NLP and the Semantic Web. Prompt engineering has proven to be a key enabler – from simple prompts that yield basic class/property listings to complex multi-turn prompts that emulate an expert’s thought process ￼ ￼. Ensuring the OWL compliance of outputs requires marrying LLMs with semantic tools (RDF/OWL libraries, validators, reasoners) and sometimes looping the LLM into a self-correction process ￼ ￼. The evaluation of these methods is multi-dimensional, combining automatic metrics with human judgment to fully appraise ontology quality ￼ ￼.

For practitioners interested in exploring these techniques, there are several resources and toolkits available:
• Ontology Generation with LLMs (ESWC 2025) – Lippolis et al.’s work (and their GitHub repo) provides prompts and code for the Memoryless CQ-by-CQ and Ontogenia techniques ￼ ￼. It’s a great starting point to see how to structure prompts for requirements-driven ontology creation, and includes evaluation scripts and dataset.
• OntoGenix (open-source GUI) – A toolkit that uses GPT-4 to semi-automatically generate OWL ontologies from CSV datasets ￼. It shows a practical, data-driven ontology building process and how prompts can be crafted for specific subtasks (summarizing data, suggesting ontology structure, generating OWL code). The repository includes example prompt templates for refining class definitions ￼.
• “Automating Knowledge Graph Creation with LLMs” (Medium post) – An end-to-end tutorial by Grigoryan (2024) demonstrating ontology generation, triple extraction, and iterative refinement using Python libraries (LangChain, Owlready2, RDFlib) and GPT-4 ￼ ￼. It provides code snippets for each step, which can be adapted to various domains.
• LLMs4OL Challenge papers – These (2024–2025) papers present several approaches to LLM-based ontology learning, often with open-source model examples. For instance, one paper uses open LLMs like Mistral-24B and Qwen-7B with chain-of-thought prompts for term and class extraction in multiple domains ￼ ￼. Another (Bakker et al. 2023) assesses GPT outputs on news data, highlighting current strengths and weaknesses ￼. These can be found in the CEUR workshop proceedings and provide insight into how LLMs perform in the wild.

In summary, LLMs are showing great promise in both populating and constructing ontologies with minimal human input. They excel at turning natural language into structured OWL representations, which is valuable for general knowledge modeling in social sciences, education, etc. By combining smart prompts, supportive tools (RDFLib, Protégé, LangChain, etc.), and rigorous validation, one can achieve a workflow where ontologies are built and filled in a fraction of the time it used to take – all while maintaining alignment with OWL formalisms and domain requirements. As research continues (e.g. adding reasoning-enabled LLMs ￼ ￼ or fine-tuned models for ontology tasks), we can expect even more robust methods to emerge, making ontology engineering a more automated and accessible practice.

Sources: The information above is drawn from recent studies and implementations, including academic papers (ESWC 2025, ISWC 2025 challenge) and open-source projects. Key references include Lippolis et al. (2025) on ontology generation techniques ￼ ￼, Shimizu et al. (2024) on knowledge graph population with LLMs ￼ ￼, the Medium tutorial by Grigoryan (2024) ￼ ￼, and the OntoGenix GitHub toolkit ￼, among others. Each provides further examples of prompts, code, and results for those interested in deeper exploration.
