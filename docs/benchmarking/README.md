# Production Benchmarks

Comprehensive benchmark suite for the Effect Ontology knowledge graph extraction system.

## Quick Links

ðŸ“‹ **[Specification](../docs/PRODUCTION_BENCHMARK_SPECIFICATION.md)** - What we're building and why  
ðŸ”§ **[Implementation Guide](../docs/BENCHMARK_IMPLEMENTATION_GUIDE.md)** - How to build it  
ðŸ“Š **[Handoff Summary](../docs/BENCHMARK_HANDOFF_SUMMARY.md)** - Executive overview

## Quick Start

```bash
# 1. Download datasets
bun run benchmark:download

# 2. Run quick validation (100 samples, ~10 min)
bun run benchmark:quick

# 3. View results
cat benchmarks/results/latest.json | jq '.metrics'
```

## Structure

```
benchmarks/
â”œâ”€â”€ datasets/           # Downloaded benchmark data
â”‚   â”œâ”€â”€ webnlg/        # WebNLG text-to-RDF benchmark
â”‚   â”œâ”€â”€ adversarial/   # Synthetic edge cases
â”‚   â””â”€â”€ cross-domain/  # Domain generalization tests
â”‚
â”œâ”€â”€ scripts/           # Automation scripts
â”‚   â”œâ”€â”€ download-datasets.sh
â”‚   â”œâ”€â”€ run-benchmarks.sh
â”‚   â””â”€â”€ generate-reports.sh
â”‚
â”œâ”€â”€ src/               # TypeScript implementation
â”‚   â”œâ”€â”€ data/         # Dataset loaders and parsers
â”‚   â”œâ”€â”€ evaluation/   # Metrics calculation
â”‚   â”œâ”€â”€ baselines/    # Baseline systems for comparison
â”‚   â””â”€â”€ reporting/    # Report generation
â”‚
â”œâ”€â”€ results/          # Benchmark outputs (JSON)
â”‚   â”œâ”€â”€ correctness/
â”‚   â”œâ”€â”€ robustness/
â”‚   â”œâ”€â”€ efficiency/
â”‚   â””â”€â”€ baselines/
â”‚
â””â”€â”€ reports/          # Generated reports (Markdown, HTML)
    â”œâ”€â”€ weekly-metrics.md
    â”œâ”€â”€ baseline-comparison.md
    â””â”€â”€ regression-history.json
```

## Usage

### Development Workflow

```bash
# Smoke test (10 samples, < 2 min)
bun run benchmark:smoke

# Quick mode (100 samples, ~10 min)
bun run benchmark:quick

# Full evaluation (1000+ samples, ~3 hours)
bun run benchmark:full

# Adversarial tests
bun run benchmark:adversarial

# Generate reports
bun run benchmark:report
```

### CI/CD Integration

Benchmarks run automatically:

- **On PR:** Quick mode (100 samples)
- **Weekly:** Full mode (1000+ samples)
- **Manual:** Via GitHub Actions workflow

### Metrics

We track three tiers:

1. **Correctness** - F1, Precision, Recall on standard benchmarks
2. **Robustness** - Performance under adversarial conditions
3. **Efficiency** - Throughput, latency, cost

## Success Criteria

âœ… **Competitive:** F1 > 0.75 on WebNLG  
âœ… **Robust:** Robustness score > 0.85  
âœ… **Efficient:** Cost < $0.01/doc, throughput > 1 doc/sec  
âœ… **Better than baselines:** Beat zero-shot LLM by 10+ F1 points

## Current Status

**Functional tests:** 32/32 passing âœ…  
**Production benchmarks:** In development ðŸš§

See [Handoff Summary](../docs/BENCHMARK_HANDOFF_SUMMARY.md) for implementation timeline.

## Contributing

See [Implementation Guide](../docs/BENCHMARK_IMPLEMENTATION_GUIDE.md) for detailed instructions.

## License

Same as parent project.
