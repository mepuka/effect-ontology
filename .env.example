# Effect Ontology Configuration
# Copy this file to .env and fill in your actual values

# =============================================================================
# LLM Configuration
# =============================================================================

# LLM Provider Selection
# Valid values: "anthropic" | "gemini" | "openrouter"
LLM__PROVIDER=anthropic

# -----------------------------------------------------------------------------
# Anthropic Configuration (Claude)
# -----------------------------------------------------------------------------
# Get your API key from: https://console.anthropic.com/
LLM__ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Model selection (optional, default: claude-3-5-sonnet-20241022)
# Available models:
# - claude-3-5-sonnet-20241022 (recommended for production)
# - claude-3-5-haiku-20241022 (faster, cheaper)
# - claude-3-opus-20240229 (most capable, slower)
LLM__ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Max tokens for responses (optional, default: 4096)
LLM__ANTHROPIC_MAX_TOKENS=4096

# Temperature for generation (optional, default: 0.0)
# Range: 0.0 (deterministic) to 1.0 (creative)
LLM__ANTHROPIC_TEMPERATURE=0.0

# -----------------------------------------------------------------------------
# Google Gemini Configuration
# -----------------------------------------------------------------------------
# Get your API key from: https://makersuite.google.com/app/apikey
LLM__GEMINI_API_KEY=your-gemini-api-key-here

# Model selection (optional, default: gemini-2.0-flash-exp)
# Available models:
# - gemini-2.0-flash-exp (recommended for fast responses)
# - gemini-1.5-pro (most capable)
# - gemini-1.5-flash (balanced)
LLM__GEMINI_MODEL=gemini-2.0-flash-exp

# Max tokens for responses (optional, default: 4096)
LLM__GEMINI_MAX_TOKENS=4096

# Temperature for generation (optional, default: 0.0)
LLM__GEMINI_TEMPERATURE=0.0

# -----------------------------------------------------------------------------
# OpenRouter Configuration
# -----------------------------------------------------------------------------
# Get your API key from: https://openrouter.ai/keys
LLM__OPENROUTER_API_KEY=your-openrouter-api-key-here

# Model selection (optional, default: anthropic/claude-3.5-sonnet)
# See available models: https://openrouter.ai/models
# Examples:
# - anthropic/claude-3.5-sonnet
# - google/gemini-2.0-flash-exp
# - openai/gpt-4-turbo
LLM__OPENROUTER_MODEL=anthropic/claude-3.5-sonnet

# Max tokens for responses (optional, default: 4096)
LLM__OPENROUTER_MAX_TOKENS=4096

# Temperature for generation (optional, default: 0.0)
LLM__OPENROUTER_TEMPERATURE=0.0

# OpenRouter-specific headers (optional)
LLM__OPENROUTER_SITE_URL=https://your-app.com
LLM__OPENROUTER_SITE_NAME=YourAppName

# =============================================================================
# RDF Configuration (N3 Service)
# =============================================================================

# RDF serialization format (optional, default: Turtle)
# Valid values: "Turtle" | "N-Triples" | "N-Quads" | "TriG"
RDF__FORMAT=Turtle

# Base IRI for relative references (optional)
RDF__BASE_IRI=http://example.org/

# Custom namespace prefixes can be added programmatically
# Default prefixes are provided:
# - rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns#
# - rdfs: http://www.w3.org/2000/01/rdf-schema#
# - xsd: http://www.w3.org/2001/XMLSchema#
# - foaf: http://xmlns.com/foaf/0.1/
# - dcterms: http://purl.org/dc/terms/

# =============================================================================
# SHACL Configuration (Future)
# =============================================================================

# Enable SHACL validation (optional, default: false)
SHACL__ENABLED=false

# Path to SHACL shapes file (optional)
SHACL__SHAPES_PATH=./shapes/ontology.ttl

# Strict mode - fail on validation errors (optional, default: true)
SHACL__STRICT_MODE=true

# =============================================================================
# Notes
# =============================================================================
#
# 1. Environment Variable Naming:
#    - Use double underscores (__) for nested configs (Effect Config convention)
#    - Example: LLM__ANTHROPIC_API_KEY maps to Config.nested("LLM")(Config.string("ANTHROPIC_API_KEY"))
#
# 2. Provider Selection:
#    - Only configure the provider you're using
#    - If LLM__PROVIDER=anthropic, only LLM__ANTHROPIC_* vars are required
#
# 3. Security:
#    - Never commit .env to version control
#    - Keep API keys secret and rotate them regularly
#    - Use environment-specific .env files (.env.production, .env.development)
#
# 4. Testing:
#    - Use programmatic config in tests (see Config/Services.ts)
#    - Example: makeLlmTestConfig({ provider: "anthropic", ... })
#
