# Effect Ontology Configuration
# Copy this file to .env and fill in your actual values

# =============================================================================
# LLM Configuration
# =============================================================================
#
# IMPORTANT: This project has TWO sets of environment variables:
#
# 1. Backend (LLM.*) - Used by Node/Bun scripts, tests, and backend services
#    - Standard naming: LLM.PROVIDER, LLM.ANTHROPIC_API_KEY, etc.
#    - Accessed via process.env in Node/Bun environments
#
# 2. Frontend (VITE_LLM_*) - Used by the browser UI via Vite
#    - VITE_ prefixed: VITE_LLM_PROVIDER, VITE_LLM_ANTHROPIC_API_KEY, etc.
#    - Accessed via import.meta.env in browser code
#    - Only VITE_* variables are exposed to the browser by Vite
#
# For development, set BOTH versions with the same values.
# For production, consider using the Settings UI instead of .env for frontend.
#
# =============================================================================

# -----------------------------------------------------------------------------
# LLM Provider Selection
# -----------------------------------------------------------------------------
# Valid values: "anthropic" | "openai" | "gemini" | "openrouter"

# Backend
LLM.PROVIDER=anthropic

# Frontend (VITE_ prefix required for browser access)
VITE_LLM_PROVIDER=anthropic

# -----------------------------------------------------------------------------
# Anthropic Configuration (Claude)
# -----------------------------------------------------------------------------
# Get your API key from: https://console.anthropic.com/

# Backend
LLM.ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Frontend
VITE_LLM_ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Model selection (optional, default: claude-3-5-sonnet-20241022)
# Available models (from @effect/ai-anthropic):
# Latest (2025):
# - claude-3-7-sonnet-20250219 (newest Sonnet variant)
# - claude-sonnet-4-5-20250929 (latest Sonnet 4.5)
# - claude-opus-4-1-20250805 (most capable - $15/$75 per 1M tokens)
# - claude-haiku-4-5-20251001 (fastest - $1/$5 per 1M tokens)
# Recommended for Production:
# - claude-3-5-sonnet-20241022 (proven, stable - $3/$15 per 1M tokens) **DEFAULT**
# - claude-3-5-haiku-20241022 (budget option - $1/$5 per 1M tokens)
# Legacy:
# - claude-3-opus-20240229 (Claude 3 Opus)
# - claude-3-haiku-20240307 (Claude 3 Haiku)

# Backend
LLM.ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Frontend
VITE_LLM_ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Max tokens for responses (optional, default: 4096)

# Backend
LLM.ANTHROPIC_MAX_TOKENS=4096

# Frontend
VITE_LLM_ANTHROPIC_MAX_TOKENS=4096

# Temperature for generation (optional, default: 0.0)
# Range: 0.0 (deterministic) to 1.0 (creative)
# For structured extraction: use 0.0 for consistency

# Backend
LLM.ANTHROPIC_TEMPERATURE=0.0

# Frontend
VITE_LLM_ANTHROPIC_TEMPERATURE=0.0

# -----------------------------------------------------------------------------
# OpenAI Configuration (GPT models)
# -----------------------------------------------------------------------------
# Get your API key from: https://platform.openai.com/api-keys

# Backend
LLM.OPENAI_API_KEY=your-openai-api-key-here

# Frontend
VITE_LLM_OPENAI_API_KEY=your-openai-api-key-here

# Model selection (optional, default: gpt-4o)
# Available models (from @effect/ai-openai):
# Latest (2025):
# - gpt-5-2025-08-07 (GPT-5 latest)
# - gpt-5-mini-2025-08-07 (GPT-5 mini)
# - gpt-4.1-2025-04-14 (GPT-4.1 latest)
# - o3-2025-04-16 (reasoning model)
# - o3-mini-2025-01-31 (reasoning model mini)
# Current Recommended:
# - gpt-4o (best balance - $2.50/$10 per 1M tokens) **DEFAULT**
# - gpt-4o-2024-11-20 (specific snapshot)
# - gpt-4o-mini (budget option - $0.15/$0.60 per 1M tokens)
# - gpt-4o-mini-2024-07-18 (specific snapshot)
# Reasoning Models:
# - o1 (latest O1 - advanced reasoning)
# - o1-mini (smaller reasoning model)
# Legacy (not recommended):
# - gpt-4-turbo (replaced by gpt-4o)
# - gpt-3.5-turbo (deprecated - use gpt-4o-mini instead)

# Backend
LLM.OPENAI_MODEL=gpt-4o

# Frontend
VITE_LLM_OPENAI_MODEL=gpt-4o

# Max tokens for responses (optional, default: 4096)

# Backend
LLM.OPENAI_MAX_TOKENS=4096

# Frontend
VITE_LLM_OPENAI_MAX_TOKENS=4096

# Temperature for generation (optional, default: 0.0)
# For structured extraction: use 0.0 for consistency

# Backend
LLM.OPENAI_TEMPERATURE=0.0

# Frontend
VITE_LLM_OPENAI_TEMPERATURE=0.0

# -----------------------------------------------------------------------------
# Google Gemini Configuration
# -----------------------------------------------------------------------------
# Get your API key from: https://makersuite.google.com/app/apikey

# Backend
LLM.GEMINI_API_KEY=your-gemini-api-key-here

# Frontend
VITE_LLM_GEMINI_API_KEY=your-gemini-api-key-here

# Model selection (optional, default: gemini-2.5-flash)
# Available models (January 2025):
# Latest (2025):
# - gemini-2.5-flash (recommended - $0.30/$2.50 per 1M tokens, 1M context) **DEFAULT**
# - gemini-2.5-pro (high-quality - $1.25/$10 per 1M tokens)
# - gemini-2.5-flash-lite (budget - $0.10/$0.40 per 1M tokens)
# Previous Generation:
# - gemini-2.0-flash-exp (experimental - being phased out)
# - gemini-1.5-pro (legacy - use 2.5 instead)
# - gemini-1.5-flash (legacy - use 2.5 instead)
# Note: Gemini provides up to 1M token context window

# Backend
LLM.GEMINI_MODEL=gemini-2.5-flash

# Frontend
VITE_LLM_GEMINI_MODEL=gemini-2.5-flash

# Max tokens for responses (optional, default: 4096)

# Backend
LLM.GEMINI_MAX_TOKENS=4096

# Frontend
VITE_LLM_GEMINI_MAX_TOKENS=4096

# Temperature for generation (optional, default: 0.0)
# For structured extraction: use 0.0 for consistency

# Backend
LLM.GEMINI_TEMPERATURE=0.0

# Frontend
VITE_LLM_GEMINI_TEMPERATURE=0.0

# -----------------------------------------------------------------------------
# OpenRouter Configuration
# -----------------------------------------------------------------------------
# Get your API key from: https://openrouter.ai/keys

# Backend
LLM.OPENROUTER_API_KEY=your-openrouter-api-key-here

# Frontend
VITE_LLM_OPENROUTER_API_KEY=your-openrouter-api-key-here

# Model selection (optional, default: anthropic/claude-3.5-sonnet)
# See available models: https://openrouter.ai/models
# Examples:
# - anthropic/claude-3.5-sonnet
# - google/gemini-2.0-flash-exp
# - openai/gpt-4-turbo

# Backend
LLM.OPENROUTER_MODEL=anthropic/claude-3.5-sonnet

# Frontend
VITE_LLM_OPENROUTER_MODEL=anthropic/claude-3.5-sonnet

# Max tokens for responses (optional, default: 4096)

# Backend
LLM.OPENROUTER_MAX_TOKENS=4096

# Frontend
VITE_LLM_OPENROUTER_MAX_TOKENS=4096

# Temperature for generation (optional, default: 0.0)

# Backend
LLM.OPENROUTER_TEMPERATURE=0.0

# Frontend
VITE_LLM_OPENROUTER_TEMPERATURE=0.0

# OpenRouter-specific headers (optional - backend only, not used in frontend)
LLM.OPENROUTER_SITE_URL=https://your-app.com
LLM.OPENROUTER_SITE_NAME=YourAppName

# =============================================================================
# RDF Configuration (N3 Service)
# =============================================================================

# RDF serialization format (optional, default: Turtle)
# Valid values: "Turtle" | "N-Triples" | "N-Quads" | "TriG"
RDF._FORMAT=Turtle

# Base IRI for relative references (optional)
RDF._BASE_IRI=http://example.org/

# Custom namespace prefixes can be added programmatically
# Default prefixes are provided:
# - rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns#
# - rdfs: http://www.w3.org/2000/01/rdf-schema#
# - xsd: http://www.w3.org/2001/XMLSchema#
# - foaf: http://xmlns.com/foaf/0.1/
# - dcterms: http://purl.org/dc/terms/

# =============================================================================
# OpenTelemetry Tracing Configuration
# =============================================================================

# Enable/disable tracing (optional, default: true)
TRACING_ENABLED=true

# Jaeger endpoint for trace export (optional, default: http://localhost:14268/api/traces)
# Change this to match your Jaeger deployment
JAEGER_ENDPOINT=http://localhost:14268/api/traces

# =============================================================================
# SHACL Configuration (Future)
# =============================================================================

# Enable SHACL validation (optional, default: false)
SHACL.ENABLED=false

# Path to SHACL shapes file (optional)
SHACL.SHAPES_PATH=./shapes/ontology.ttl

# Strict mode - fail on validation errors (optional, default: true)
SHACL.STRICT_MODE=true

# =============================================================================
# Notes
# =============================================================================
#
# 1. Environment Variable Naming:
#    - Use double underscores (__) for nested configs (Effect Config convention)
#    - Example: LLM.ANTHROPIC_API_KEY maps to Config.nested("LLM")(Config.string("ANTHROPIC_API_KEY"))
#
# 2. Provider Selection:
#    - Only configure the provider you're using
#    - If LLM.PROVIDER=anthropic, only LLM.ANTHROPIC_* vars are required
#
# 3. Security:
#    - Never commit .env to version control
#    - Keep API keys secret and rotate them regularly
#    - Use environment-specific .env files (.env.production, .env.development)
#
# 4. Testing:
#    - Use programmatic config in tests (see Config/Services.ts)
#    - Example: makeLlmTestConfig({ provider: "anthropic", ... })
#
