# Effect Ontology Configuration
# Copy this file to .env and fill in your actual values

# =============================================================================
# LLM Configuration
# =============================================================================

# LLM Provider Selection
# Valid values: "anthropic" | "openai" | "gemini" | "openrouter"
LLM__PROVIDER=anthropic

# -----------------------------------------------------------------------------
# Anthropic Configuration (Claude)
# -----------------------------------------------------------------------------
# Get your API key from: https://console.anthropic.com/
LLM__ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Model selection (optional, default: claude-3-5-sonnet-20241022)
# Available models (from @effect/ai-anthropic):
# Latest (2025):
# - claude-3-7-sonnet-20250219 (newest Sonnet variant)
# - claude-sonnet-4-5-20250929 (latest Sonnet 4.5)
# - claude-opus-4-1-20250805 (most capable - $15/$75 per 1M tokens)
# - claude-haiku-4-5-20251001 (fastest - $1/$5 per 1M tokens)
# Recommended for Production:
# - claude-3-5-sonnet-20241022 (proven, stable - $3/$15 per 1M tokens) **DEFAULT**
# - claude-3-5-haiku-20241022 (budget option - $1/$5 per 1M tokens)
# Legacy:
# - claude-3-opus-20240229 (Claude 3 Opus)
# - claude-3-haiku-20240307 (Claude 3 Haiku)
LLM__ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Max tokens for responses (optional, default: 4096)
LLM__ANTHROPIC_MAX_TOKENS=4096

# Temperature for generation (optional, default: 0.0)
# Range: 0.0 (deterministic) to 1.0 (creative)
# For structured extraction: use 0.0 for consistency
LLM__ANTHROPIC_TEMPERATURE=0.0

# -----------------------------------------------------------------------------
# OpenAI Configuration (GPT models)
# -----------------------------------------------------------------------------
# Get your API key from: https://platform.openai.com/api-keys
LLM__OPENAI_API_KEY=your-openai-api-key-here

# Model selection (optional, default: gpt-4o)
# Available models (from @effect/ai-openai):
# Latest (2025):
# - gpt-5-2025-08-07 (GPT-5 latest)
# - gpt-5-mini-2025-08-07 (GPT-5 mini)
# - gpt-4.1-2025-04-14 (GPT-4.1 latest)
# - o3-2025-04-16 (reasoning model)
# - o3-mini-2025-01-31 (reasoning model mini)
# Current Recommended:
# - gpt-4o (best balance - $2.50/$10 per 1M tokens) **DEFAULT**
# - gpt-4o-2024-11-20 (specific snapshot)
# - gpt-4o-mini (budget option - $0.15/$0.60 per 1M tokens)
# - gpt-4o-mini-2024-07-18 (specific snapshot)
# Reasoning Models:
# - o1 (latest O1 - advanced reasoning)
# - o1-mini (smaller reasoning model)
# Legacy (not recommended):
# - gpt-4-turbo (replaced by gpt-4o)
# - gpt-3.5-turbo (deprecated - use gpt-4o-mini instead)
LLM__OPENAI_MODEL=gpt-4o

# Max tokens for responses (optional, default: 4096)
LLM__OPENAI_MAX_TOKENS=4096

# Temperature for generation (optional, default: 0.0)
# For structured extraction: use 0.0 for consistency
LLM__OPENAI_TEMPERATURE=0.0

# -----------------------------------------------------------------------------
# Google Gemini Configuration
# -----------------------------------------------------------------------------
# Get your API key from: https://makersuite.google.com/app/apikey
LLM__GEMINI_API_KEY=your-gemini-api-key-here

# Model selection (optional, default: gemini-2.5-flash)
# Available models (January 2025):
# Latest (2025):
# - gemini-2.5-flash (recommended - $0.30/$2.50 per 1M tokens, 1M context) **DEFAULT**
# - gemini-2.5-pro (high-quality - $1.25/$10 per 1M tokens)
# - gemini-2.5-flash-lite (budget - $0.10/$0.40 per 1M tokens)
# Previous Generation:
# - gemini-2.0-flash-exp (experimental - being phased out)
# - gemini-1.5-pro (legacy - use 2.5 instead)
# - gemini-1.5-flash (legacy - use 2.5 instead)
# Note: Gemini provides up to 1M token context window
LLM__GEMINI_MODEL=gemini-2.5-flash

# Max tokens for responses (optional, default: 4096)
LLM__GEMINI_MAX_TOKENS=4096

# Temperature for generation (optional, default: 0.0)
# For structured extraction: use 0.0 for consistency
LLM__GEMINI_TEMPERATURE=0.0

# -----------------------------------------------------------------------------
# OpenRouter Configuration
# -----------------------------------------------------------------------------
# Get your API key from: https://openrouter.ai/keys
LLM__OPENROUTER_API_KEY=your-openrouter-api-key-here

# Model selection (optional, default: anthropic/claude-3.5-sonnet)
# See available models: https://openrouter.ai/models
# Examples:
# - anthropic/claude-3.5-sonnet
# - google/gemini-2.0-flash-exp
# - openai/gpt-4-turbo
LLM__OPENROUTER_MODEL=anthropic/claude-3.5-sonnet

# Max tokens for responses (optional, default: 4096)
LLM__OPENROUTER_MAX_TOKENS=4096

# Temperature for generation (optional, default: 0.0)
LLM__OPENROUTER_TEMPERATURE=0.0

# OpenRouter-specific headers (optional)
LLM__OPENROUTER_SITE_URL=https://your-app.com
LLM__OPENROUTER_SITE_NAME=YourAppName

# =============================================================================
# RDF Configuration (N3 Service)
# =============================================================================

# RDF serialization format (optional, default: Turtle)
# Valid values: "Turtle" | "N-Triples" | "N-Quads" | "TriG"
RDF__FORMAT=Turtle

# Base IRI for relative references (optional)
RDF__BASE_IRI=http://example.org/

# Custom namespace prefixes can be added programmatically
# Default prefixes are provided:
# - rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns#
# - rdfs: http://www.w3.org/2000/01/rdf-schema#
# - xsd: http://www.w3.org/2001/XMLSchema#
# - foaf: http://xmlns.com/foaf/0.1/
# - dcterms: http://purl.org/dc/terms/

# =============================================================================
# SHACL Configuration (Future)
# =============================================================================

# Enable SHACL validation (optional, default: false)
SHACL__ENABLED=false

# Path to SHACL shapes file (optional)
SHACL__SHAPES_PATH=./shapes/ontology.ttl

# Strict mode - fail on validation errors (optional, default: true)
SHACL__STRICT_MODE=true

# =============================================================================
# Notes
# =============================================================================
#
# 1. Environment Variable Naming:
#    - Use double underscores (__) for nested configs (Effect Config convention)
#    - Example: LLM__ANTHROPIC_API_KEY maps to Config.nested("LLM")(Config.string("ANTHROPIC_API_KEY"))
#
# 2. Provider Selection:
#    - Only configure the provider you're using
#    - If LLM__PROVIDER=anthropic, only LLM__ANTHROPIC_* vars are required
#
# 3. Security:
#    - Never commit .env to version control
#    - Keep API keys secret and rotate them regularly
#    - Use environment-specific .env files (.env.production, .env.development)
#
# 4. Testing:
#    - Use programmatic config in tests (see Config/Services.ts)
#    - Example: makeLlmTestConfig({ provider: "anthropic", ... })
#
