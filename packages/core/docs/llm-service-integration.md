# LlmService Integration Guide

## Overview

The `LlmService` provides LLM-powered knowledge graph extraction using `@effect/ai`'s LanguageModel with structured output generation. It integrates seamlessly with the existing Prompt service and Schema generation.

## Architecture

```
┌─────────────────┐
│  Input Text     │
└────────┬────────┘
         │
         ├──────────────────┐
         │                  │
┌────────▼────────┐  ┌──────▼──────┐
│  OntologyContext│  │PromptAlgebra│
│  (Graph)        │  │             │
└────────┬────────┘  └──────┬──────┘
         │                  │
         │             ┌────▼────────┐
         │             │solveGraph() │
         │             │             │
         │             └────┬────────┘
         │                  │
         │           ┌──────▼───────────┐
         │           │StructuredPrompt  │
         │           │ {system, user,   │
         │           │  examples}       │
         │           └──────┬───────────┘
         │                  │
┌────────▼──────────────────▼────────┐
│  extractVocabulary()                │
│  → classIris, propertyIris          │
└────────┬────────────────────────────┘
         │
┌────────▼─────────────────┐
│  makeKnowledgeGraphSchema│
│  (with vocabulary)        │
└────────┬─────────────────┘
         │
┌────────▼──────────────────┐
│  LlmService               │
│  .extractKnowledgeGraph() │
│                            │
│  1. buildPromptText()     │
│  2. LanguageModel.        │
│     generateObject()      │
│  3. Return validated KG   │
└────────┬──────────────────┘
         │
┌────────▼─────────┐
│  KnowledgeGraph  │
│  {entities: [...]}│
└──────────────────┘
```

## Complete Pipeline Example

```typescript
import { Effect } from "effect"
import { LanguageModel } from "@effect/ai"
import { OpenAI } from "@effect/ai-openai" // or Anthropic
import { LlmService } from "@effect-ontology/core/Services/Llm"
import { RdfService } from "@effect-ontology/core/Services/Rdf"
import { solveGraph } from "@effect-ontology/core/Prompt/Solver"
import { defaultPromptAlgebra } from "@effect-ontology/core/Prompt/Algebra"
import { makeKnowledgeGraphSchema } from "@effect-ontology/core/Schema/Factory"
import { extractVocabulary } from "@effect-ontology/core/Services/Llm"

const extractionPipeline = (
  text: string,
  ontologyGraph: Graph.Graph<NodeId, unknown, "directed">,
  ontology: OntologyContext
) =>
  Effect.gen(function* () {
    // Step 1: Generate prompts for each node
    const prompts = yield* solveGraph(
      ontologyGraph,
      ontology,
      defaultPromptAlgebra
    )

    // For simplicity, use a single root prompt
    const rootPrompt = prompts.get(rootNodeId)!

    // Step 2: Extract vocabulary and create schema
    const { classIris, propertyIris } = extractVocabulary(ontology)
    const schema = makeKnowledgeGraphSchema(classIris, propertyIris)

    // Step 3: Extract knowledge graph using LLM
    const llm = yield* LlmService
    const knowledgeGraph = yield* llm.extractKnowledgeGraph(
      text,
      ontology,
      rootPrompt,
      schema
    )

    // Step 4: Convert to RDF
    const rdf = yield* RdfService
    const store = yield* rdf.jsonToStore(knowledgeGraph)

    // Step 5: Serialize to Turtle (optional)
    const turtle = yield* rdf.storeToTurtle(store)

    return {
      knowledgeGraph,
      store,
      turtle
    }
  })

// Run the pipeline
const program = extractionPipeline(
  "Alice is a person who knows Bob.",
  graph,
  ontology
).pipe(
  Effect.provide(LlmService.Default),
  Effect.provide(RdfService.Default),
  Effect.provide(OpenAI.layer({ apiKey: "..." })) // Provide actual LLM
)

Effect.runPromise(program)
```

## API Reference

### `LlmService`

#### `extractKnowledgeGraph`

```typescript
extractKnowledgeGraph<ClassIRI extends string, PropertyIRI extends string>(
  text: string,
  ontology: OntologyContext,
  prompt: StructuredPrompt,
  schema: KnowledgeGraphSchema<ClassIRI, PropertyIRI>
): Effect.Effect<
  KnowledgeGraph<ClassIRI, PropertyIRI>,
  LLMError,
  LanguageModel
>
```

**Parameters:**
- `text` - The input text to extract knowledge from
- `ontology` - The ontology context (currently unused, reserved for future extensions)
- `prompt` - Structured prompt from Prompt service (via `solveGraph`)
- `schema` - Dynamic schema generated by `makeKnowledgeGraphSchema`

**Returns:**
- `Effect` yielding validated `KnowledgeGraph` or `LLMError`
- Requires `LanguageModel` service in context

### `extractVocabulary`

```typescript
extractVocabulary(ontology: OntologyContext): {
  classIris: string[]
  propertyIris: string[]
}
```

**Purpose:**
Helper function to extract class and property IRIs from an `OntologyContext` for schema generation.

**Example:**
```typescript
const { classIris, propertyIris } = extractVocabulary(ontology)
const schema = makeKnowledgeGraphSchema(classIris, propertyIris)
```

## Error Handling

All errors are wrapped in `LLMError`:

```typescript
class LLMError extends S.TaggedError<LLMError>(
  "@effect-ontology/Extraction/LLMError"
)("LLMError", {
  module: S.String,
  method: S.String,
  reason: S.Literal("ApiError", "ApiTimeout", "InvalidResponse", "ValidationFailed"),
  description: S.optional(S.String),
  cause: S.optional(S.Unknown)
}) {}
```

**Error Recovery:**
```typescript
const program = llm.extractKnowledgeGraph(text, ontology, prompt, schema).pipe(
  Effect.catchTag("LLMError", (error) => {
    if (error.reason === "ApiTimeout") {
      // Retry logic
      return Effect.retry(program, Schedule.recurs(3))
    }
    return Effect.fail(error)
  })
)
```

## Provider Integration

### OpenAI

```typescript
import { OpenAI } from "@effect/ai-openai"

const layer = OpenAI.layer({
  apiKey: Config.string("OPENAI_API_KEY")
})

const program = extractionPipeline(text, graph, ontology).pipe(
  Effect.provide(layer),
  Effect.provide(LlmService.Default),
  Effect.provide(RdfService.Default)
)
```

### Anthropic

```typescript
import { Anthropic } from "@effect/ai-anthropic"

const layer = Anthropic.layer({
  apiKey: Config.string("ANTHROPIC_API_KEY")
})

const program = extractionPipeline(text, graph, ontology).pipe(
  Effect.provide(layer),
  Effect.provide(LlmService.Default),
  Effect.provide(RdfService.Default)
)
```

## Testing

Use mock LanguageModel layers for testing:

```typescript
import { Layer } from "effect"
import { LanguageModel } from "@effect/ai"

const MockLanguageModelLayer = Layer.effect(
  LanguageModel,
  LanguageModel.make({
    generateText: () =>
      Effect.succeed([
        { type: "text", text: '{"entities": []}' },
        { type: "finish", reason: "stop", usage: {...} }
      ]),
    streamText: () =>
      Effect.fail(new Error("Not implemented"))
  })
)

const testProgram = llm.extractKnowledgeGraph(...).pipe(
  Effect.provide(LlmService.Default),
  Effect.provide(MockLanguageModelLayer)
)
```

## Performance Considerations

1. **Token Usage**: The prompt can be large for complex ontologies. Consider:
   - Filtering unnecessary properties
   - Using prompt caching (if provider supports it)
   - Batching multiple texts per session

2. **Schema Size**: Large vocabularies increase schema complexity:
   - Limit to relevant classes/properties per extraction
   - Split large ontologies into subgraphs

3. **Validation Overhead**: Schema validation is automatic but has overhead:
   - Use `Effect.cached` for repeated extractions with the same schema
   - Pre-generate schemas for common ontology subsets

## Next Steps

See:
- `packages/core/src/Services/Llm.ts` - Implementation
- `packages/core/test/Services/Llm.test.ts` - Tests
- `packages/core/src/Prompt/Solver.ts` - Prompt generation
- `packages/core/src/Schema/Factory.ts` - Schema creation
- `packages/core/src/Services/Rdf.ts` - RDF conversion
